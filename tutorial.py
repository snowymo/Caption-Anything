# -*- coding: utf-8 -*-
"""tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ttengwang/Caption-Anything/blob/main/notebooks/tutorial.ipynb

# Caption Anything

In this notebook, we'll illustrate the usage of [Caption Anything](https://github.com/ttengwang/Caption-Anything) step by step.

## Introduction

Caption-Anything is a versatile image processing tool that combines the capabilities of [Segment Anything](https://github.com/facebookresearch/segment-anything), Visual Captioning, and [ChatGPT](https://openai.com/blog/chatgpt). Our solution generates descriptive captions for any object within an image, offering a range of language styles to accommodate diverse user preferences. Caption-Anything supports visual controls (mouse click) and language controls (length, sentiment, factuality, and language).

## Set-up environment
We'll start by installing HuggingFace Transformers library from source as the model is brand new at the time of writing this notebook.
"""

# !pip install git+https://github.com/huggingface/transformers.git
# !pip install git+https://github.com/facebookresearch/segment-anything.git
import sys
sys.path.append("../transformers")
sys.path.append("../segment-anything")

"""## Load image

Let's load the image from the Meta AI's [Segment Anything Demo](https://segment-anything.com/demo) or from local device.
"""

import requests
from PIL import Image

# from Segment Anything demo
url = './GettyImages-1207721867.jpg'
image = Image.open(url).convert('RGB')

# # from local device
# img_path = './demo.jpg'
# image = Image.open(img_path).convert('RGB')

width, height = image.size
# display(image.resize((width // 3, height // 3)))
print("w",width,"h",height)

"""## Load segmenter

Before using the Segment Anything Model(SAM), let's download the checkpoint of SAM and save it to the current directory.
"""

# !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

"""Next, we leverage the downloaded checkpoint to initialize the SAM, where 'vit_h' is chosen as the backbone in our example.

It is highly recommended to run SAM in GPU! It will make generation faster.
"""

import torch
import numpy as np
from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator

# parameters
device = "cuda" if torch.cuda.is_available() else "cpu"
print("device", device)
torch_dtype = torch.float16 if 'cuda' in device else torch.float32
model_type = 'vit_h'
checkpoint = 'sam_vit_h_4b8939.pth'

# SAM initialization
model = sam_model_registry[model_type](checkpoint = checkpoint)
model.to(device)
predictor = SamPredictor(model)
mask_generator = SamAutomaticMaskGenerator(model)
predictor.set_image(np.array(image)) # load the image to predictor

# model

"""SAM produces high quality object masks from input prompts such as points or boxes, and even masks. Here, we take click prompt (i.e., points) as and example."""

input_point = [[1800, 950]] # A Nx2 array of point prompts to the model. Each point is in (X,Y) in pixels.
input_label = [1]           # A length N array of labels for the point prompts. 1 indicates a foreground point and 0 indicates a background point.
input_point = np.array(input_point)
input_label = np.array(input_label)
masks, scores, logits = predictor.predict(point_coords = input_point, point_labels = input_label)
masks = masks[0, ...]

# display(Image.fromarray(masks).resize(((width // 3, height // 3))))
mask = Image.fromarray(masks)
def save_img(masks,name):
    # 将 masks 的值域从 [0, 1] 转换为 [0, 255] 并转换为整数
    masks_scaled = (masks * 255).astype(np.uint8)
    # 将缩放后的数组转换为图像
    img = Image.fromarray(masks_scaled)
    # 保存图像
    img.save(name)
save_img(mask,'mask.png')

"""## Crop the image

we next crop the masked objects from the origin image generated by SAM.

To crop the image, we should choose whether to remain the background of the image firstly!
"""

crop_mode = "wo_bg" # Optional['wo_bg', 'w_bg'], where w_bg and wo_bg refer to remain and discard background separately.

if crop_mode == "wo_bg":
    masked_image = image * masks[:,:,np.newaxis] + (1 - masks[:,:,np.newaxis]) * 255
    masked_image = np.uint8(masked_image)
else:
    masked_image = np.array(image)
masked_image = Image.fromarray(masked_image)
save_img(masked_image,'masked_image.png')

# display(masked_image.resize((width // 3, height // 3)))

"""Then, we use a regular box to cut the objects from the image as an example.

Note that the masks is a 2D matrix where 1 indicates a foreground point and 0 indicates a background point. To obtain the regular box containing masked objects, we reshape the 2D matrix to an vector, and the first 1 (A) appearing in this vector must be the point on the upper boundary of regular box. We can prove this conclusion through proof to the contrary. We suppose A is not the upper boundary of regular box which means that there is a higher foreground point (B) in the masks. When we reshape masks to an vector, B must appear before A, which is contradictory with suppose.

Once we obtain the index of A in the vector, we divide it by the number of the rows to get the position of boundary. Similarly, we can get the lower, left and right boundary too!
"""

def boundary(inputs):

    col = inputs.shape[1]
    inputs = inputs.reshape(-1)
    lens = len(inputs)
    start = np.argmax(inputs)
    end = lens - 1 - np.argmax(np.flip(inputs))
    top = start // col
    bottom = end // col

    return top, bottom

def seg_to_box(seg_mask, size):

    top, bottom = boundary(seg_mask)
    left, right = boundary(seg_mask.T)
    left, top, right, bottom = left / size, top / size, right / size, bottom / size # we normalize the size of boundary to 0 ~ 1

    return [left, top, right, bottom]

size = max(masks.shape[0], masks.shape[1])
left, top, right, bottom = seg_to_box(masks, size) # calculating the position of the top-left and bottom-right corners in the image
print(left, top, right, bottom)

image_crop = masked_image.crop((left * size, top * size, right * size, bottom * size)) # crop the image
image_crop.save("image_crop.jpg")
# display(image_crop)

"""## Load captioning model

Here, we leverage BLIP2 as the captioning model. In order to accelarate the inference speed, let us start by install accelerate and bitsandbytes library to enable model acceleration and int8 quantization algorithm.
"""

# !pip install accelerate bitsandbytes

"""Let's load a BLIP-2 checkpoint that leverages the pre-trained OPT model by Meta AI. We choose 2.7-billion-parameters BLIP2 as an example, for more scales, refer to: https://huggingface.co/spaces/Salesforce/BLIP2."""

from transformers import AutoProcessor, Blip2ForConditionalGeneration

processor = AutoProcessor.from_pretrained("../blip2-opt-2.7b")
captioning_model = Blip2ForConditionalGeneration.from_pretrained("../blip2-opt-2.7b", device_map = "sequential", load_in_8bit = True)

"""Now, everything is ready, let's caption now! Defaultly, BLIP2 will start generating caption related to the image from the BOS token."""

inputs = processor(image_crop, return_tensors = "pt").to(device, torch_dtype)
out = captioning_model.generate(**inputs, max_new_tokens = 50)
captions = processor.decode(out[0], skip_special_tokens = True).strip()

print("captions",captions)

"""Optionally, text prompt can be provided to guide the process of text generation."""

# text_prompt = 'Question: What does the image show? Answer:'
#
# inputs = processor(image_crop, text = text_prompt, return_tensors = "pt").to(device, torch_dtype)
# out = captioning_model.generate(**inputs, max_new_tokens = 50)
# captions = processor.decode(out[0], skip_special_tokens = True).strip()